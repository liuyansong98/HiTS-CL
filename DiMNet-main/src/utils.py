"""
Utility functions for link prediction
Most code is adapted from authors' implementation of RGCN link prediction:
https://github.com/MichSchli/RelationPrediction

"""
import numpy as np
import torch
from tqdm import tqdm
import kgutil as knwlgrh
from collections import defaultdict
from functools import reduce
import random
import os
from torch import distributed as dist
from torch_geometric.data import Data


def set_rand_seed(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def get_rank():
    if dist.is_initialized():
        return dist.get_rank()
    if "RANK" in os.environ:
        return int(os.environ["RANK"])
    return 0


def get_world_size():
    if dist.is_initialized():
        return dist.get_world_size()
    if "WORLD_SIZE" in os.environ:
        return int(os.environ["WORLD_SIZE"])
    return 1


def synchronize():
    if get_world_size() > 1:
        dist.barrier()


def get_device(args):
    if args.gpus:
        device = torch.device(args.gpus[get_rank()])
    else:
        device = torch.device("cpu")
    return device


def create_working_directory(args, current_time):
    file_name = "working_dir" + current_time + ".tmp"
    world_size = get_world_size()
    if args.gpus is not None and len(args.gpus) != world_size:
        error_msg = "World size is %d but found %d GPUs in the argument"
        if world_size == 1:
            error_msg += ". Did you launch with `python -m torch.distributed.launch`?"
        raise ValueError(error_msg % (world_size, len(args.gpus)))
    if world_size > 1 and not dist.is_initialized():
        dist.init_process_group("nccl", init_method="env://")

    working_dir = os.path.join("../model", args.dataset)

    # synchronize working directory
    if get_rank() == 0:
        with open(file_name, "w") as fout:
            fout.write(working_dir)
        if not os.path.exists(working_dir):
            os.makedirs(working_dir)
    synchronize()
    if get_rank() != 0:
        with open(file_name, "r") as fin:
            working_dir = fin.read()
    synchronize()
    if get_rank() == 0:
        os.remove(file_name)

    os.chdir(working_dir)
    return working_dir


#######################################################################
#
# Utility function for building training and testing graphs
#
#######################################################################


def edge_match(edge_index, query_index):
    # O((n + q)logn) time
    # O(n) memory
    # edge_index: big underlying graph
    # query_index: edges to match

    # preparing unique hashing of edges, base: (max_node, max_relation) + 1
    base = edge_index.max(dim=1)[0] + 1
    # we will map edges to long ints, so we need to make sure the maximum product is less than MAX_LONG_INT
    # idea: max number of edges = num_nodes * num_relations
    # e.g. for a graph of 10 nodes / 5 relations, edge IDs 0...9 mean all possible outgoing edge types from node 0
    # given a tuple (h, r), we will search for all other existing edges starting from head h
    assert reduce(int.__mul__, base.tolist()) < torch.iinfo(torch.long).max
    scale = base.cumprod(0)
    # scale = scale[-1] // scale
    scale = torch.div(scale[-1], scale, rounding_mode='floor')

    # hash both the original edge index and the query index to unique integers
    edge_hash = (edge_index * scale.unsqueeze(-1)).sum(dim=0)
    edge_hash, order = edge_hash.sort()
    query_hash = (query_index * scale.unsqueeze(-1)).sum(dim=0)

    # matched ranges: [start[i], end[i])
    start = torch.bucketize(query_hash, edge_hash)
    end = torch.bucketize(query_hash, edge_hash, right=True)
    # num_match shows how many edges satisfy the (h, r) pattern for each query in the batch
    num_match = end - start

    # generate the corresponding ranges
    offset = num_match.cumsum(0) - num_match
    range = torch.arange(num_match.sum(), device=edge_index.device)
    range = range + (start - offset).repeat_interleave(num_match)

    return order[range], num_match


def negative_sampling(all_data, batch_data, num_negative, num_nodes, num_rels, strict=True):
    all_data_t = all_data
    batch_t = batch_data
    data = {
        'num_nodes': num_nodes,
        'edge_index': torch.stack([all_data_t[:, 0], all_data_t[:, 1]]),
        'edge_type': all_data_t[:, 2]
    }

    batch_size = len(batch_t)
    pos_h_index, pos_t_index, pos_r_index = batch_t.t()

    # strict negative sampling vs random negative sampling
    if strict:
        t_mask, h_mask = strict_negative_mask(data, batch_t)

        t_mask = t_mask[:batch_size // 2]
        neg_t_candidate = t_mask.nonzero()[:, 1]
        num_t_candidate = t_mask.sum(dim=-1)
        # draw samples for negative tails
        rand = torch.rand(len(t_mask), num_negative, device=batch_t.device)
        index = (rand * num_t_candidate.unsqueeze(-1)).long()
        index = index + (num_t_candidate.cumsum(0) - num_t_candidate).unsqueeze(-1)
        neg_t_index = neg_t_candidate[index]

        h_mask = h_mask[batch_size // 2:]
        neg_h_candidate = h_mask.nonzero()[:, 1]
        num_h_candidate = h_mask.sum(dim=-1)
        # draw samples for negative heads
        rand = torch.rand(len(h_mask), num_negative, device=batch_t.device)
        index = (rand * num_h_candidate.unsqueeze(-1)).long()
        index = index + (num_h_candidate.cumsum(0) - num_h_candidate).unsqueeze(-1)
        neg_h_index = neg_h_candidate[index]
    else:
        neg_index = torch.randint(data['num_nodes'], (batch_size, num_negative), device=batch_t.device)
        neg_t_index, neg_h_index = neg_index[:batch_size // 2], neg_index[batch_size // 2:]

    h_index = pos_h_index.unsqueeze(-1).repeat(1, num_negative + 1)
    t_index = pos_t_index.unsqueeze(-1).repeat(1, num_negative + 1)
    r_index = pos_r_index.unsqueeze(-1).repeat(1, num_negative + 1)
    t_index[:batch_size // 2, 1:] = neg_t_index
    h_index[batch_size // 2:, 1:] = neg_h_index

    return torch.stack([h_index, t_index, r_index], dim=-1)


def strict_negative_mask(data, batch):
    # this function makes sure that for a given (h, r) batch we will NOT sample true tails as random negatives
    # similarly, for a given (t, r) we will NOT sample existing true heads as random negatives

    pos_h_index, pos_t_index, pos_r_index = batch.t()

    # part I: sample hard negative tails
    # edge index of all (head, relation) edges from the underlying graph
    edge_index = torch.stack([data['edge_index'][0], data['edge_type']])
    # edge index of current batch (head, relation) for which we will sample negatives
    query_index = torch.stack([pos_h_index, pos_r_index])
    # search for all true tails for the given (h, r) batch
    edge_id, num_t_truth = edge_match(edge_index, query_index)
    # build an index from the found edges
    t_truth_index = data['edge_index'][1, edge_id]
    sample_id = torch.arange(len(num_t_truth), device=batch.device).repeat_interleave(num_t_truth)
    t_mask = torch.ones(len(num_t_truth), data['num_nodes'], dtype=torch.bool, device=batch.device)
    # assign 0s to the mask with the found true tails
    t_mask[sample_id, t_truth_index] = 0
    t_mask.scatter_(1, pos_t_index.unsqueeze(-1), 0)

    # part II: sample hard negative heads
    # edge_index[1] denotes tails, so the edge index becomes (t, r)
    edge_index = torch.stack([data['edge_index'][1], data['edge_type']])
    # edge index of current batch (tail, relation) for which we will sample heads
    query_index = torch.stack([pos_t_index, pos_r_index])
    # search for all true heads for the given (t, r) batch
    edge_id, num_h_truth = edge_match(edge_index, query_index)
    # build an index from the found edges
    h_truth_index = data['edge_index'][0, edge_id]
    sample_id = torch.arange(len(num_h_truth), device=batch.device).repeat_interleave(num_h_truth)
    h_mask = torch.ones(len(num_h_truth), data['num_nodes'], dtype=torch.bool, device=batch.device)
    # assign 0s to the mask with the found true heads
    h_mask[sample_id, h_truth_index] = 0
    h_mask.scatter_(1, pos_h_index.unsqueeze(-1), 0)

    return t_mask, h_mask


def all_negative(num_nodes, batch):
    pos_h_index, pos_t_index, pos_r_index = batch.t()
    r_index = pos_r_index.unsqueeze(-1).expand(-1, num_nodes)
    # generate all negative tails for this batch
    all_index = torch.arange(num_nodes, device=batch.device)
    h_index, t_index = torch.meshgrid(pos_h_index, all_index)
    t_batch = torch.stack([h_index, t_index, r_index], dim=-1)
    # generate all negative heads for this batch
    all_index = torch.arange(num_nodes, device=batch.device)
    t_index, h_index = torch.meshgrid(pos_t_index, all_index)
    h_batch = torch.stack([h_index, t_index, r_index], dim=-1)

    return t_batch, h_batch


def generate_graph_data(data, num_nodes, num_rels, stat_show=False, device=None):
    # change the view of the data
    # [[s,r,o,t],[s,r,o,t],[s,r,o,t],...] -->> [ [ [s,r,o],[s,r,o] ], [ [s,r,o] ],...]
    train_list_sp = split_by_time(data.train, stat_show)
    valid_list_sp = split_by_time(data.valid, stat_show)
    test_list_sp = split_by_time(data.test, stat_show)

    # generate graph list
    # [ [ [s,r,o],[s,r,o] ], [ [s,r,o] ],...] -->> [ g1, g2, g3, ... ]
    train_graph_list = build_graph(num_nodes, num_rels, train_list_sp, device)
    valid_graph_list = build_graph(num_nodes, num_rels, valid_list_sp, device)
    test_graph_list = build_graph(num_nodes, num_rels, test_list_sp, device)

    return train_list_sp, valid_list_sp, test_list_sp, train_graph_list, valid_graph_list, test_graph_list


def build_graph(num_nodes, num_rels, snapshot_list, device):
    graph_list = []
    for snapshot in snapshot_list:
        torch_snapshot = torch.from_numpy(snapshot).long()
        src, rel, dst, ts = torch_snapshot.transpose(0, 1)
        # add reverse edges
        src, dst = torch.cat((src, dst)), torch.cat((dst, src))
        rel = torch.cat((rel, rel + num_rels))

        graph = Data(edge_index=torch.stack([src, dst]), edge_type=rel,
                     num_nodes=num_nodes, target_triplets=torch_snapshot[:, [0, 2, 1]])
        graph = graph.to(device)
        graph_list.append(graph)
    return graph_list


def get_total_rank(valid_pred, valid_triple, num_nodes):
    valid_time_filter_data = {
        'num_nodes': num_nodes,
        'edge_index': torch.stack([valid_triple[:, 0], valid_triple[:, 1]]),
        'edge_type': valid_triple[:, 2]
    }
    valid_src, valid_dst, valid_r = valid_triple.t()
    timef_t_mask, timef_h_mask = strict_negative_mask(valid_time_filter_data, valid_triple)
    rank_raw, rank_filter = compute_ranking(valid_pred, valid_dst, timef_t_mask)
    mrr_filter_snap = torch.mean(1.0 / rank_filter.float()).item()
    mrr_snap = torch.mean(1.0 / rank_raw.float()).item()
    return mrr_filter_snap, mrr_snap, rank_raw, rank_filter


def stat_ranks(rank_list, method, mode, dataset=""):  # added  eval_paper_authors log for logging
    hits = [1, 3, 10]
    total_rank = torch.cat(rank_list)
    mrr = torch.mean(1.0 / total_rank.float())
    print("MRR ({}): {:.6f}".format(method, mrr.item()))
    mrr_list = []
    testSet_rank = []
    for hit in hits:
        avg_count = torch.mean((total_rank <= hit).float())
        print("Hits ({}) @ {}: {:.6f}".format(method, hit, avg_count.item()))
    for i in range(len(rank_list)):
        mrr_list.append(torch.mean(1.0 / rank_list[i].float()).item())
        # print(mrr_list[i])
    mrr_list_id = np.arange(len(mrr_list))
    x_bar = np.mean(mrr_list_id)
    y_bar = np.mean(np.array(mrr_list))
    decay = np.sum((np.array(mrr_list) - y_bar) * (mrr_list_id - x_bar)) / np.sum((mrr_list_id - x_bar) ** 2)
    print(f"{mode} decay: {decay}\n")

    return mrr, mrr_list

def stat_ranks_clod(rank_list, method, mode, dataset=""):  # added  eval_paper_authors log for logging
    hits = [1, 3, 10]
    total_rank = torch.tensor(rank_list)
    mrr = torch.mean(1.0 / total_rank.float())
    print("MRR ({}): {:.6f}".format(method, mrr.item()))
    mrr_list = []
    testSet_rank = []
    for hit in hits:
        avg_count = torch.mean((total_rank <= hit).float())
        print("Hits ({}) @ {}: {:.6f}".format(method, hit, avg_count.item()))
    for i in range(len(rank_list)):
        mrr_list.append(torch.mean(1.0 / rank_list[i].float()).item())
        # print(mrr_list[i])
    mrr_list_id = np.arange(len(mrr_list))
    x_bar = np.mean(mrr_list_id)
    y_bar = np.mean(np.array(mrr_list))
    decay = np.sum((np.array(mrr_list) - y_bar) * (mrr_list_id - x_bar)) / np.sum((mrr_list_id - x_bar) ** 2)
    print(f"{mode} decay: {decay}\n")

    return mrr, mrr_list

def split_by_time(data, stat_show=False):
    snapshot_list = []
    snapshot = []
    snapshots_num = 0
    latest_t = 0
    for i in range(len(data)):
        t = data[i][3]
        train = data[i]

        if latest_t != t:
            # show snapshot
            latest_t = t
            if len(snapshot):
                snapshot_list.append(np.array(snapshot).copy())
                snapshots_num += 1
            snapshot = []
        if train not in snapshot:
            snapshot.append(train)

    if len(snapshot) > 0:
        snapshot_list.append(np.array(snapshot).copy())
        snapshots_num += 1

    if stat_show:
        nodes = []
        rels = []
        for snapshot in snapshot_list:
            uniq_v, edges = np.unique((snapshot[:, 0], snapshot[:, 2]), return_inverse=True)  # relabel
            uniq_r = np.unique(snapshot[:, 1])
            edges = np.reshape(edges, (2, -1))
            nodes.append(len(uniq_v))
            rels.append(len(uniq_r) * 2)

        print(
            "# Sanity Check:  ave node num : {:04f}, ave rel num : {:04f}, snapshots num: {:04d}, max edges num: {:04d}, min edges num: {:04d}"
            .format(np.average(np.array(nodes)), np.average(np.array(rels)), len(snapshot_list),
                    max([len(_) for _ in snapshot_list]), min([len(_) for _ in snapshot_list])))
    return snapshot_list


def load_data(dataset, bfs_level=3, relabel=False):
    if dataset in ['aifb', 'mutag', 'bgs', 'am']:
        return knwlgrh.load_entity(dataset, bfs_level, relabel)
    elif dataset in ['FB15k', 'wn18', 'FB15k-237']:
        return knwlgrh.load_link(dataset)
    else :
        return knwlgrh.load_from_local("../../data", dataset)

def compute_ranking(pred, target, mask):
    pos_pred = pred.gather(-1, target.unsqueeze(-1))
    # filtered ranking
    ranking_filter = torch.sum((pos_pred <= pred) & mask, dim=-1) + 1
    # unfiltered ranking
    ranking_raw = torch.sum(pos_pred < pred, dim=-1) + 1
    return ranking_raw, ranking_filter
